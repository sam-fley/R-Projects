[
  {
    "objectID": "Project-Model-Selection-Encelia.html",
    "href": "Project-Model-Selection-Encelia.html",
    "title": "Encelia Flower Model Selection",
    "section": "",
    "text": "Southern California is home to two native species in the Encelia genus, californica and farinosa. Encelia californica is found in more coastal regions and Encelia farinosa is found in more inland desert regions.\nIt has become common in Orange County to plant farinosa near roadsides and in other areas where californica is native. The two plants hybridize easily, so it is not uncommon to see in the wild a plant that has some characteristics of californica and some characteristics of farinosa.\nThe Fullerton Arboretum is home to both species. Our goal is to build a model that can “discriminate” between californica and farinosa. Such models might later be used by botanists to investigate how a putative hybrid can be discriminated from the two parent species."
  },
  {
    "objectID": "Project-Model-Selection-Encelia.html#motivation-and-context",
    "href": "Project-Model-Selection-Encelia.html#motivation-and-context",
    "title": "Encelia Flower Model Selection",
    "section": "",
    "text": "Southern California is home to two native species in the Encelia genus, californica and farinosa. Encelia californica is found in more coastal regions and Encelia farinosa is found in more inland desert regions.\nIt has become common in Orange County to plant farinosa near roadsides and in other areas where californica is native. The two plants hybridize easily, so it is not uncommon to see in the wild a plant that has some characteristics of californica and some characteristics of farinosa.\nThe Fullerton Arboretum is home to both species. Our goal is to build a model that can “discriminate” between californica and farinosa. Such models might later be used by botanists to investigate how a putative hybrid can be discriminated from the two parent species."
  },
  {
    "objectID": "Project-Model-Selection-Encelia.html#main-objective",
    "href": "Project-Model-Selection-Encelia.html#main-objective",
    "title": "Encelia Flower Model Selection",
    "section": "Main Objective",
    "text": "Main Objective\nThe goal of this project is to create a few different classification models that predict if a flower is californica or farinosa. After these models have been made, we will perform model selection on them to determine which model is “best”."
  },
  {
    "objectID": "Project-Model-Selection-Encelia.html#packages-used-in-this-analysis",
    "href": "Project-Model-Selection-Encelia.html#packages-used-in-this-analysis",
    "title": "Encelia Flower Model Selection",
    "section": "Packages Used In This Analysis",
    "text": "Packages Used In This Analysis\n\nlibrary(here)\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(rsample)\nlibrary(purrr)\nlibrary(yardstick)\nlibrary(tidyr)\nlibrary(broom)\nlibrary(patchwork)\nlibrary(gt)\n\n\n\n\nPackage\nUse\n\n\n\n\nhere\nto easily load and save data\n\n\nreadr\nto import the CSV file data\n\n\ndplyr\nto massage and summarize data\n\n\nggplot2\nto create nice-looking and informative graphs\n\n\nrsample\nto split data into training and test sets\n\n\npurrr\nto run the cross-validation\n\n\nyardstick\nto evalute the accuracy of the models\n\n\ntidyr\nto “pivot” the predictions data frame so that each row represents 1 model\n\n\nbroom\nto create tibbles\n\n\npatchwork\nto graph multiple graphs at once\n\n\ngt\nto create tables"
  },
  {
    "objectID": "Project-Model-Selection-Encelia.html#design-and-data-collection",
    "href": "Project-Model-Selection-Encelia.html#design-and-data-collection",
    "title": "Encelia Flower Model Selection",
    "section": "Design and Data Collection",
    "text": "Design and Data Collection\nThis data was collected during an in class trip to the Fullerton Arboretum. Students decided collectively on what data to collect, then broke into groups to collect this data. The class decide to measure the number of rays, ray length (overall diameter), disk length (disk diameter), and stem length.\n\nThere were some limitations to our data collection method. One major difficulty was that since we did not designate sections of the Arboretum for each group to measure, flowers most likely got measured multiple times. Another limitation would be that each student could read their ruler slightly differently, leading to a variation in the data."
  },
  {
    "objectID": "Project-Model-Selection-Encelia.html#training-test-split",
    "href": "Project-Model-Selection-Encelia.html#training-test-split",
    "title": "Encelia Flower Model Selection",
    "section": "Training-Test Split",
    "text": "Training-Test Split\nThere isn’t much data massaging we need to do here, because we were very deliberate about how we set up our data sheet and how we recorded the data on it.\nAt the end of this section, you should write code to randomly split the Encelia data into a training and test set and explain why a training and test set are useful/necessary for this objective.\nWe will create a training and test set of our data. This will be useful in our modeling step by allowing us to test the effectiveness of our models by predicting on our test set. Since we will know the actual species vs the predicted species, we can evaluate how well a model performs.\n\nencelia &lt;- encelia |&gt;\n  mutate(\n    Species = as.factor(Species),\n    Species = recode(Species, `C` = 'Californica', `F` = 'Farinosa')\n  ) \n  \n\nset.seed(69)\n\nencelia_split &lt;- initial_split(\n  encelia,\n  strata = Species,\n  prop = 0.80\n)\n\nencelia_train &lt;- training(encelia_split) \nencelia_test &lt;- testing(encelia_split)"
  },
  {
    "objectID": "Project-Model-Selection-Encelia.html#exploratory-data-analysis",
    "href": "Project-Model-Selection-Encelia.html#exploratory-data-analysis",
    "title": "Encelia Flower Model Selection",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nFor our exploratory data analysis, we will be creating four different graphs. Each graph will group by the species and display one of our measurements.\nFirst, we will look at the number of rays.\n\nggplot(\n  data = encelia_train,\n  mapping = aes(x = number_rays, y = Species)\n) +\n  geom_jitter(height = 0.1) +\n  labs(\n    title = \"Number of Rays by Species\",\n    x = \"Number of Rays\",\n    y = \"\"\n  ) +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(\n      hjust = 0.5, # center\n      face = \"bold\",\n      size = 20\n    ),\n    axis.title.x = element_text( # sets only x-axis\n      size = 15\n    ),\n    axis.text.x = element_text(\n      size = 12\n    ),\n    axis.title.y = element_text( # sets only y-axis\n      size = 15\n    ),\n    axis.text.y = element_text(\n      size = 15\n    )\n        )\n\n\n\n\nAs shown above, farinosa seems to have less variance in amount of rays compared to californica. This difference could lead to ray numbers being a good predictor variable for californica.\nNext, we will graph disk length.\n\nggplot(\n  data = encelia_train,\n  mapping = aes(x = disk_diameter, y = Species)\n) +\n  geom_jitter(height = 0.1) +\n    labs(\n    title = \"Disk Diameter by Species\",\n    x = \"Disk Diameter (cm)\",\n    y = \"\"\n  ) +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(\n      hjust = 0.5, # center\n      face = \"bold\",\n      size = 20\n    ),\n    axis.title.x = element_text( # sets only x-axis\n      size = 15\n    ),\n    axis.text.x = element_text(\n      size = 12\n    ),\n    axis.title.y = element_text( # sets only y-axis\n      size = 15\n    ),\n    axis.text.y = element_text(\n      size = 15\n    )\n        )\n\n\n\n\nAs we can see, disk diameter between californica and farinosa seems to have similar variance. However, both species are centered in different places, which means that this variable could be a good predictor between species.\nNow, we will look at ray length.\n\nggplot(\n  data = encelia_train,\n  mapping = aes(x = ray_diameter, y = Species)\n) +\n  geom_jitter(height = 0.1) +\n    labs(\n    title = \"Ray Diameter by Species\",\n    x = \"Ray Diameter (cm)\",\n    y = \"\"\n  ) +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(\n      hjust = 0.5, # center\n      face = \"bold\",\n      size = 20\n    ),\n    axis.title.x = element_text( # sets only x-axis\n      size = 15\n    ),\n    axis.text.x = element_text(\n      size = 12\n    ),\n    axis.title.y = element_text( # sets only y-axis\n      size = 15\n    ),\n    axis.text.y = element_text(\n      size = 15\n    )\n        )\n\n\n\n\nRay diameter seems to be similar to ray number. Both have farinosa with a smaller variance with an overlap between the two species.\nFinally, we will graph stem length.\n\nggplot(\n  data = encelia_train,\n  mapping = aes(x = stem_length, y = Species)\n) +\n  geom_jitter(height = 0.1) +\n    labs(\n    title = \"Stem Length by Species\",\n    x = \"Stem Length (cm)\",\n    y = \"\"\n  ) +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(\n      hjust = 0.5, # center\n      face = \"bold\",\n      size = 20\n    ),\n    axis.title.x = element_text( # sets only x-axis\n      size = 15\n    ),\n    axis.text.x = element_text(\n      size = 12\n    ),\n    axis.title.y = element_text( # sets only y-axis\n      size = 15\n    ),\n    axis.text.y = element_text(\n      size = 15\n    )\n        )\n\n\n\n\nAs shown, farinosa has a small variance and is offset from californica. This variable would most likely be a good predictor between species."
  },
  {
    "objectID": "Project-Model-Selection-Encelia.html#modeling",
    "href": "Project-Model-Selection-Encelia.html#modeling",
    "title": "Encelia Flower Model Selection",
    "section": "Modeling",
    "text": "Modeling\nPropose several logistic regression models and use cross-validation to select a best model, following the steps in the “Cross-Validation (Part 3)” video and the “Model Selection for Logistic Regression” activity.\nWe will make 4 different models, 1 of which will be the null model. We will then perform cross-validation to select a “best” model. We will be making these models with logistic regression. Logistic regression is a statistical model that models the log-odds of an event as a linear combination of one or more independent variables.\nI decided to make a model with all the variables, just ray data, just lengths, and a null model. I decided to make these models since these models pair data that seem like they could work in conjunction with each other.\n\nmodel_1 &lt;- glm(\n  Species ~ number_rays + disk_diameter + ray_diameter + stem_length,\n  data = encelia_train,\n  family = \"binomial\"\n)\n\n\nmodel_1_pred &lt;- model_1 |&gt;\n  augment(newdata = encelia_test,\n          type.predict = \"response\")\n\nmodel_1_pred &lt;- model_1_pred |&gt;\n  mutate(\n    .pred_californica = 1 - .fitted,\n    .pred_farinosa = .fitted,\n    .pred_class = if_else(\n      .fitted &lt; 0.5,\n      \"Californica\",\n      \"Farinosa\"\n    ) |&gt;\n      as.factor()\n  )\n\n\nmodel_2 &lt;- glm(\n  Species ~ number_rays + ray_diameter,\n  data = encelia_train,\n  family = \"binomial\"\n)\n\n\nmodel_2_pred &lt;- model_2 |&gt;\n  augment(newdata = encelia_test,\n          type.predict = \"response\")\n\nmodel_2_pred &lt;- model_2_pred |&gt;\n  mutate(\n    .pred_californica = 1 - .fitted,\n    .pred_farinosa = .fitted,\n    .pred_class = if_else(\n      .fitted &lt; 0.5,\n      \"Californica\",\n      \"Farinosa\"\n    ) |&gt;\n      as.factor()\n  )\n\n\nmodel_3 &lt;- glm(\n  Species ~ disk_diameter + ray_diameter + stem_length,\n  data = encelia_train,\n  family = \"binomial\"\n)\n\n\nmodel_3_pred &lt;- model_3 |&gt;\n  augment(newdata = encelia_test,\n          type.predict = \"response\")\n\nmodel_3_pred &lt;- model_3_pred |&gt;\n  mutate(\n    .pred_californica = 1 - .fitted,\n    .pred_farinosa = .fitted,\n    .pred_class = if_else(\n      .fitted &lt; 0.5,\n      \"Californica\",\n      \"Farinosa\"\n    ) |&gt;\n      as.factor()\n  )\n\n\nnull_model &lt;- glm(\n  Species ~ 1,\n  data = encelia_train,\n  family = \"binomial\"\n)\n\n\nnull_model_pred &lt;- null_model |&gt;\n  augment(newdata = encelia_test,\n          type.predict = \"response\")\n\nnull_model_pred &lt;- null_model_pred |&gt;\n  mutate(\n    .pred_californica = 1 - .fitted,\n    .pred_farinosa = .fitted,\n    .pred_class = if_else(\n      .fitted &lt; 0.5,\n      \"Californica\",\n      \"Farinosa\"\n    ) |&gt;\n      as.factor()\n  )\n\nNow that we have all of our models, we can now plot our predictions.\n\nggplot_1 &lt;- ggplot(\n  data = model_1_pred,\n  mapping = aes(x = .pred_farinosa, y = Species)\n) +\n  geom_dotplot() +\n  scale_x_continuous(labels = scales::percent) +\n      labs(\n    title = \"Farinosa Prediction \\n Confidence (Model 1)\",\n    x = \"Predictions (Percent)\",\n    y = \"\"\n  ) +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(\n      hjust = 0.5, # center\n      face = \"bold\",\n      size = 15\n    ),\n    axis.title.x = element_text( # sets only x-axis\n      size = 15\n    ),\n    axis.text.x = element_text(\n      size = 12\n    ),\n    axis.title.y = element_text( # sets only y-axis\n      size = 15\n    ),\n    axis.text.y = element_text(\n      size = 15\n    ) \n        )\n\n\nggplot_2 &lt;- ggplot(\n  data = model_2_pred,\n  mapping = aes(x = .pred_farinosa, y = Species)\n) +\n  geom_dotplot() +\n  scale_x_continuous(labels = scales::percent) +\n      labs(\n    title = \"Farinosa Prediction \\n Confidence (Model 2)\",\n    x = \"Predictions (Percent)\",\n    y = \"\"\n  ) +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(\n      hjust = 0.5, # center\n      face = \"bold\",\n      size = 15\n    ),\n    axis.title.x = element_text( # sets only x-axis\n      size = 15\n    ),\n    axis.text.x = element_text(\n      size = 12\n    ),\n    axis.title.y = element_text( # sets only y-axis\n      size = 15\n    ),\n    axis.text.y = element_text(\n      size = 15\n    ) \n        )\n\n\nggplot_3 &lt;- ggplot(\n  data = model_3_pred,\n  mapping = aes(x = .pred_farinosa, y = Species)\n) +\n  geom_dotplot() +\n  scale_x_continuous(labels = scales::percent) +\n      labs(\n    title = \"Farinosa Prediction \\n Confidence (Model 3)\",\n    x = \"Predictions (Percent)\",\n    y = \"\"\n  ) +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(\n      hjust = 0.5, # center\n      face = \"bold\",\n      size = 15\n    ),\n    axis.title.x = element_text( # sets only x-axis\n      size = 15\n    ),\n    axis.text.x = element_text(\n      size = 12\n    ),\n    axis.title.y = element_text( # sets only y-axis\n      size = 15\n    ),\n    axis.text.y = element_text(\n      size = 15\n    ) \n        )\n\n\n\nggplot_1 + ggplot_2 + ggplot_3 +\n  plot_layout(ncol = 2)\n\n\n\n\nThese graphs show the confidence of farinosa predictions compared to the actual species. Of these graphs, model 1 seems to be doing the best at differentiating between the two species.\nWe will now evaluate the brier scores for each model. The brier score measures the accuracy of our predictions by calculating the mean squared difference between our predicted probability and our actual outcome. Since we are calculating mean squared difference, lower brier scores are perfered, and we expect a score of 0.25 for our null model.\n\nmodel_1_brier &lt;- brier_class(model_1_pred, Species, .pred_californica, na_rm = TRUE)\nmodel_2_brier &lt;- brier_class(model_2_pred, Species, .pred_californica, na_rm = TRUE)\nmodel_3_brier &lt;- brier_class(model_3_pred, Species, .pred_californica, na_rm = TRUE)\nnull_model_brier &lt;- brier_class(null_model_pred, Species, .pred_californica, na_rm = TRUE)\n\nbrier_scores &lt;- rbind(model_1_brier, model_2_brier, model_3_brier, null_model_brier)\nbrier_scores &lt;- brier_scores |&gt;\n  transmute(Model = c(\"Model 1\", \"Model 2\", \"Model 3\", \"Null Model\"),\n            Score = .estimate)\n\ngt(\n  brier_scores,\n  rowname_col = \"Model\"\n) |&gt;\n  tab_header(title = \"Brier Scores\") |&gt;\n  tab_source_note(source_note = \"2025 data from Class Fieldtrip\") |&gt;\n  opt_align_table_header(align = \"right\") |&gt;\n  tab_style(\n    style = list(\n      cell_fill(color = \"red\", alpha = 0.2),\n      cell_text(weight = \"bold\")\n    ),\n    locations = cells_body(\n      columns = c(Score),\n      rows = Score == min(Score)\n    )\n  )\n\n\n\n\n\n  \n    \n      Brier Scores\n    \n    \n    \n      \n      Score\n    \n  \n  \n    Model 1\n0.05447327\n    Model 2\n0.10815237\n    Model 3\n0.12322185\n    Null Model\n0.24945636\n  \n  \n    \n      2025 data from Class Fieldtrip\n    \n  \n  \n\n\n\n\nAgain model 1 seems to be our “best” model with our lowest brier score. Since model one was our “best” model in both of our model selection tests, we will select it as our “best” model."
  },
  {
    "objectID": "Project-Model-Selection-Encelia.html#insights",
    "href": "Project-Model-Selection-Encelia.html#insights",
    "title": "Encelia Flower Model Selection",
    "section": "Insights",
    "text": "Insights\nLets take a look at some incorrect predictions that models made.\n\nmodel_1_pred_false &lt;- model_1_pred |&gt; \n  select(!c(\".fitted\",\n            \".pred_californica\",\n            \".pred_farinosa\")\n         ) |&gt;\n  filter(\n    Species != .pred_class\n  )\n\ngt(\n  model_1_pred_false,\n  rowname_col = \"Species\"\n) |&gt;\n  tab_header(title = \"Incorrect Predictions\") |&gt;\n  tab_source_note(source_note = \"2025 data from Class Fieldtrip\") |&gt;\n  opt_align_table_header(align = \"right\") |&gt;\n  cols_move_to_start(.pred_class) |&gt;\n  cols_label(\n    number_rays = \"Number of Rays\",\n    disk_diameter = \"Disk Diameter\",\n    ray_diameter = \"Ray Diameter\",\n    stem_length = \"Stem Length\",\n    .pred_class = \"Prediction\"\n  )\n\n\n\n\n\n  \n    \n      Incorrect Predictions\n    \n    \n    \n      \n      Prediction\n      Number of Rays\n      Disk Diameter\n      Ray Diameter\n      Stem Length\n    \n  \n  \n    Californica\nFarinosa\n12\n1.2\n5.6\n7.2\n    Californica\nFarinosa\n14\n1.8\n4.0\n27.5\n  \n  \n    \n      2025 data from Class Fieldtrip\n    \n  \n  \n\n\n\n\nWhy were these flowers predicted incorrectly? For both of these flowers, the cause is not so obvious. For the first flower, when looking back at our exploratory data graphs where we grouped by species, we see that this flowers data lines up with the mean values for farinosa flowers. For the second flower, the most likely cause would be the ray diameter. A value of 4 cm is uncharacteristically low for californica flowers. These are the reasons that I think lead to these two flowers being predicted incorrectly."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sam Fleyshman",
    "section": "",
    "text": "I am a Mathematics major at California State University, Fullerton, with a strong interest in data science and its applications in solving real-world problems. My academic background provides a solid foundation in statistical analysis, mathematical modeling, and algorithmic thinking. I primarily work with R and have some experience with Python, using both to explore data, build models, and create visualizations. This website features my coding projects and reflects my passion for data-driven discovery and continuous learning in the field of data science."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Sam Fleyshman",
    "section": "Education",
    "text": "Education\nCalifornia State University of Fullerton | Sept 2021 - May 2025"
  },
  {
    "objectID": "hospital_analysis.html",
    "href": "hospital_analysis.html",
    "title": "Hospital Analysis",
    "section": "",
    "text": "This project aims to analyze hospital survey data to determine what effects a hospital’s quality. The hospital data that will be used was retrieved from the Centers for Medicare and Medicaid (CMS) website. This data was accessed on April 10th, 2025 and contains data from 2023 - 2024 from a multitude of different hospitals. This data contains 3 important metrics: star rating, answer percentage, and linear mean value. Of these metrics, the star rating is a 1-5 star rating given by the CMS as part of their initiative to make their Compare Web sites more intuitive. The answer percentage is the distribution of answers within each question. Most of the questions have “Always”, “Usually”, and “Sometimes or Never” as possible answers. The linear mean value is the metric that will be used for this analysis. This metric is an adjusted score accounting for patient mix, survey mode, and quarterly weighting. More details can be found here.\n\n\n\nBefore we create the model, we must first clean the data. Since only survey data is being considered, location data will not be included in this analysis. After isolating the linear mean values, we will fit a model using principal component analysis. Since our goal is inference, we already know we want to include all components, and we are not evaluating model predictions, we will not be creating a holdout set. After the model is created, we will look at the p-values of each component to determine how significant each component is."
  },
  {
    "objectID": "hospital_analysis.html#design-and-data-collection",
    "href": "hospital_analysis.html#design-and-data-collection",
    "title": "Hospital Analysis",
    "section": "",
    "text": "This project aims to analyze hospital survey data to determine what effects a hospital’s quality. The hospital data that will be used was retrieved from the Centers for Medicare and Medicaid (CMS) website. This data was accessed on April 10th, 2025 and contains data from 2023 - 2024 from a multitude of different hospitals. This data contains 3 important metrics: star rating, answer percentage, and linear mean value. Of these metrics, the star rating is a 1-5 star rating given by the CMS as part of their initiative to make their Compare Web sites more intuitive. The answer percentage is the distribution of answers within each question. Most of the questions have “Always”, “Usually”, and “Sometimes or Never” as possible answers. The linear mean value is the metric that will be used for this analysis. This metric is an adjusted score accounting for patient mix, survey mode, and quarterly weighting. More details can be found here.\n\n\n\nBefore we create the model, we must first clean the data. Since only survey data is being considered, location data will not be included in this analysis. After isolating the linear mean values, we will fit a model using principal component analysis. Since our goal is inference, we already know we want to include all components, and we are not evaluating model predictions, we will not be creating a holdout set. After the model is created, we will look at the p-values of each component to determine how significant each component is."
  },
  {
    "objectID": "hospital_analysis.html#data-massaging",
    "href": "hospital_analysis.html#data-massaging",
    "title": "Hospital Analysis",
    "section": "Data Massaging",
    "text": "Data Massaging\nSince we will not be using location data, we will remove it from our dataset.\n\nhospital &lt;- hospital |&gt;\n  clean_names()\n\nhospital_1 &lt;- hospital |&gt;\n  select(\n    !c(\"address\", \n       \"city_town\",\n       \"telephone_number\", \n       \"hcahps_answer_description\",\n       \"state\", \n       \"zip_code\", \n       \"county_parish\") # remove all location data\n  )\n\nBecause of the layout of the data, we have a lot of missing data in our three metric columns. This is the case since each entry in our question column is specific to a single metric.\n\nhospital_1_missing &lt;- head(hospital_1) |&gt;\n  select(hcahps_question, \n         patient_survey_star_rating, \n         hcahps_answer_percent, \n         hcahps_linear_mean_value)\n\ngt(\n  hospital_1_missing,\n  rowname_col = \"hcahps_question\"\n) |&gt;\n  tab_header(title = \"Glimpse of Missing Data\") |&gt;\n  tab_source_note(source_note = \"2024 data from Centers for Medicare & Medicaid Services\") |&gt;\n  opt_align_table_header(align = \"right\") |&gt;\n  cols_label(\n    patient_survey_star_rating = \"Star Rating\",\n    hcahps_answer_percent = \"Answer Percent\",\n    hcahps_linear_mean_value = \"Linear Mean Value\"\n  )\n\n\n\n\n\n  \n    \n      Glimpse of Missing Data\n    \n    \n    \n      \n      Star Rating\n      Answer Percent\n      Linear Mean Value\n    \n  \n  \n    Patients who reported that their nurses \"Always\" communicated well\nNA\n72\nNA\n    Patients who reported that their nurses \"Sometimes\" or \"Never\" communicated well\nNA\n6\nNA\n    Patients who reported that their nurses \"Usually\" communicated well\nNA\n22\nNA\n    Nurse communication - linear mean score\nNA\nNA\n89\n    Nurse communication - star rating\n2\nNA\nNA\n    Patients who reported that their nurses \"Always\" treated them with courtesy and respect\nNA\n82\nNA\n  \n  \n    \n      2024 data from Centers for Medicare & Medicaid Services\n    \n  \n  \n\n\n\n\nTo remove this missing data, we will separate the original data set into three different datasets. One for each of our metrics. Although we will only be using the linear mean value dataset, the formatting changes will be applied to all three datasets in order to keep the entire set of data in a similar format.\n\nhospital_star_rating &lt;- hospital_1 |&gt; # Star rating is a 1-5 rating of service\n  select(\n    !c(\"hcahps_answer_percent\", \"hcahps_answer_percent_footnote\", \"hcahps_linear_mean_value\" )\n  ) |&gt;\n  filter(\n    !is.na(`patient_survey_star_rating`)\n    )\n\n\nhospital_ans_perc &lt;- hospital_1 |&gt; # Answer percent is percentage of how many times that answer was chosen\n  select(\n    !c(\"patient_survey_star_rating\", \"patient_survey_star_rating_footnote\", \"hcahps_linear_mean_value\" )\n  ) |&gt;\n  filter(\n    !is.na(`hcahps_answer_percent`)\n    )\n\n\nhospital_linear_mean &lt;- hospital_1 |&gt; # linear mean is a standerdization of star rating to be out of 100\n  select(\n    !c(\"hcahps_answer_percent\", \"hcahps_answer_percent_footnote\", \"patient_survey_star_rating\", \"patient_survey_star_rating_footnote\")\n  ) |&gt;\n  filter(\n    !is.na(`hcahps_linear_mean_value`)\n    )\n\nNow, the linear mean value data is isolated from the rest.\n\nhospital_linear_mean_partial &lt;- head(hospital_linear_mean) |&gt;\n  transmute(hcahps_question, \n         hcahps_linear_mean_value\n         )\n\ngt(\n  hospital_linear_mean_partial,\n  rowname_col = \"hcahps_question\"\n) |&gt;\n  tab_header(title = \"Glimpse of Linear Mean Data\") |&gt;\n  tab_source_note(source_note = \"2024 data from Centers for Medicare & Medicaid Services\") |&gt;\n  opt_align_table_header(align = \"right\") |&gt;\n  cols_label(\n    hcahps_linear_mean_value = \"Linear Mean Value\"\n  )\n\n\n\n\n\n  \n    \n      Glimpse of Linear Mean Data\n    \n    \n    \n      \n      Linear Mean Value\n    \n  \n  \n    Nurse communication - linear mean score\n89\n    Doctor communication - linear mean score\n91\n    Staff responsiveness - linear mean score\n80\n    Communication about medicines - linear mean score\n75\n    Discharge information - linear mean score\n85\n    Care transition - linear mean score\n82\n  \n  \n    \n      2024 data from Centers for Medicare & Medicaid Services\n    \n  \n  \n\n\n\n\nHowever, in order to further analyze the data, we need to pivot the question column to create columns for each individual question.\n\nhospital_star_rating_pivot &lt;- hospital_star_rating |&gt;\n  pivot_wider(id_cols = `facility_id`,\n              names_from = `hcahps_question`,\n              values_from = `patient_survey_star_rating`) |&gt;\n  clean_names() |&gt;\n    rename_with(\n    \\(x) str_remove(x , \"_star_rating\")\n  )\n\n\nhospital_ans_perc_pivot &lt;- hospital_ans_perc |&gt;\n  pivot_wider(id_cols = `facility_id`,\n              names_from = `hcahps_question`,\n              values_from = `hcahps_answer_percent`) |&gt;\n  clean_names()\n\n\nhospital_linear_mean_pivot &lt;- hospital_linear_mean |&gt;\n  pivot_wider(id_cols = `facility_id`,\n              names_from = `hcahps_question`,\n              values_from = `hcahps_linear_mean_value`) |&gt;\n  clean_names() |&gt;\n  rename_with(\n    \\(x) str_remove(x , \"_linear_mean_score\")\n  )\n\n\nhospital_linear_mean_pivot_partial &lt;- head(hospital_linear_mean_pivot) |&gt;\n  rename(\"Quiteness\" = quietness, \n         \"Cleanliness\" = cleanliness, \n         \"Nurse Communication\" = nurse_communication, \n         \"Doctor Communication\" = doctor_communication, \n         \"Staff Responsiveness\" = staff_responsiveness, \n         \"Communication About Medicines\" = communication_about_medicines,\n         \"Discharge Information\" = discharge_information,\n         \"Care Transition\" = care_transition,\n         \"Overall Rating\" = overall_hospital_rating,\n         \"Recommend Hospital\" = recommend_hospital\n         )\n\ngt(\n  head(hospital_linear_mean_pivot_partial),\n  rowname_col = \"facility_id\"\n) |&gt;\n  tab_header(title = \"Glimpse of Pivoted Linear Mean Data\") |&gt;\n  tab_source_note(source_note = \"2024 data from Centers for Medicare & Medicaid Services\") |&gt;\n  tab_stubhead(label = \"Facility ID\") |&gt;\n  opt_align_table_header(align = \"left\")\n\n\n\n\n\n  \n    \n      Glimpse of Pivoted Linear Mean Data\n    \n    \n    \n      Facility ID\n      Nurse Communication\n      Doctor Communication\n      Staff Responsiveness\n      Communication About Medicines\n      Discharge Information\n      Care Transition\n      Cleanliness\n      Quiteness\n      Overall Rating\n      Recommend Hospital\n    \n  \n  \n    010001\n89\n91\n80\n75\n85\n82\n82\n87\n89\n90\n    010005\n90\n92\n77\n76\n87\n81\n86\n87\n87\n84\n    010006\n89\n89\n76\n70\n83\n76\n79\n87\n83\n80\n    010007\n94\n95\n89\n83\n92\n82\n83\n90\n89\n88\n    010011\n90\n90\n83\n74\n85\n80\n79\n84\n86\n86\n    010012\n93\n93\n85\n80\n87\n79\n83\n83\n85\n83\n  \n  \n    \n      2024 data from Centers for Medicare & Medicaid Services"
  },
  {
    "objectID": "hospital_analysis.html#exploratory-data-analysis",
    "href": "hospital_analysis.html#exploratory-data-analysis",
    "title": "Hospital Analysis",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nThere are two metrics that can determine a hospitals quality, overall rating and recommendation score. On the survey, individuals were asked to provide an overall rating of the hospital, and a score on how likely they would recommend the hospital. We will use the recommendation score as our indicator of quality since it offers more personalized and nuanced information, but first we need to verify that these to metrics are comparable.\n\nggplot(\n  data = hospital_linear_mean_pivot,\n  mapping = aes(x = overall_hospital_rating, y = recommend_hospital)\n) +\n  geom_point() +\n    labs(\n    title = \"Hostpital Ratings (2024)\",\n    x = \"Overall Hospital Rating\",\n    y = \"Recommendation Score\"\n  ) +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(\n      hjust = 0.5, # center\n      face = \"bold\",\n      size = 20\n    ),\n    axis.title.x = element_text( # sets only x-axis\n      size = 15\n    ),\n    axis.text.x = element_text(\n      size = 10\n    ),\n    axis.title.y = element_text( # sets only y-axis\n      size = 15\n    ),\n    axis.text.y = element_text(\n      size = 10\n    )\n        ) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\")\n\n\n\n\nThere is a positive correlation between the overall hospital rating and the recommendation score. This makes sense since a hospital with a higher rating should be more likely to be recommended. Since these metrics are comparable, we will be using the recommendation score as our indicator of quality."
  },
  {
    "objectID": "hospital_analysis.html#modeling",
    "href": "hospital_analysis.html#modeling",
    "title": "Hospital Analysis",
    "section": "Modeling",
    "text": "Modeling\nTo analyze the data and answer our question, we will create an inferential model using principal component analysis. As described in our design, we will include all components and we will not evaluate model predictions. Therefore, we will not be creating a holdout set.\n\nLinear Regression\nWe start by defining the type of model we want to run, and setting up our workflow.\n\nlinear_model &lt;- linear_reg(mode = \"regression\", engine = \"lm\")\n\nlinear_wflow &lt;- workflow() |&gt;\n  add_model(linear_model)\n\nWhen setting up the component analysis, we need to remove the overall hospital ratings. This is necessary since we have previously shown that they are directly correlated.\n\npca_recipe &lt;- recipe(\n  recommend_hospital ~ . , data = hospital_linear_mean_pivot\n) |&gt;\n## ~ . indicates to use all variables in the dataset as predictors besides the response\n  update_role(facility_id, new_role = \"id\") |&gt;\n  step_rm(overall_hospital_rating) |&gt;\n  step_normalize(all_numeric_predictors()) |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  step_pca(all_predictors(), num_comp = 8)\n\nlinear_wflow &lt;- linear_wflow |&gt;\n  add_recipe(pca_recipe)\n\nNext, we fit the model on our component analysis.\n\nlinear_fit &lt;- fit(linear_wflow, data = hospital_linear_mean_pivot)\n\nNow that we have our fitted model, we can take a look at our p-values. We will consider a p-value less than 0.01 to be significant.\n\nlinear_fit_tibble &lt;- tidy(linear_fit) |&gt;\n  arrange(p.value)\n\ngt(\n  linear_fit_tibble,\n  rowname_col = \"term\"\n) |&gt;\n  tab_header(title = \"Significance of Components\") |&gt;\n  tab_source_note(source_note = \"2024 data from Centers for Medicare & Medicaid Services\") |&gt;\n  opt_align_table_header(align = \"right\") |&gt;\n  tab_style(\n    style = list(\n      cell_fill(color = \"red\", alpha = 0.2),\n      cell_text(weight = \"bold\")\n    ),\n    locations = cells_body(\n      columns = c(estimate, p.value),\n      rows = p.value &lt;= 1e-2\n    )\n  ) |&gt;\n  cols_move_to_start(p.value) |&gt;\n  cols_label(\n    p.value = \"P-Value\",\n    estimate = \"Coefficient\",\n    std.error = \"Standard Error\",\n    statistic = \"Statistic\"\n  ) |&gt;\n  fmt_scientific(\n    columns = c(p.value)\n  )\n\n\n\n\n\n  \n    \n      Significance of Components\n    \n    \n    \n      \n      P-Value\n      Coefficient\n      Standard Error\n      Statistic\n    \n  \n  \n    (Intercept)\n0.00\n86.61088520\n0.04154645\n2084.6758732\n    PC1\n0.00\n1.74650141\n0.01709333\n102.1744309\n    PC7\n2.14 × 10−112\n-2.59640022\n0.11068064\n-23.4584865\n    PC5\n7.64 × 10−48\n-1.19465901\n0.08084693\n-14.7768004\n    PC2\n4.26 × 10−22\n0.51565222\n0.05295927\n9.7367698\n    PC6\n1.27 × 10−13\n-0.76053765\n0.10219637\n-7.4419240\n    PC4\n5.56 × 10−5\n-0.32213805\n0.07980806\n-4.0364099\n    PC8\n7.03 × 10−3\n0.37488977\n0.13898535\n2.6973331\n    PC3\n4.45 × 10−1\n-0.04300362\n0.05633992\n-0.7632887\n  \n  \n    \n      2024 data from Centers for Medicare & Medicaid Services\n    \n  \n  \n\n\n\n\nAs shown in the table, every component except for component number 3 is significant with a p-value less than 0.01. Now that we know which components significantly affect recommendation scores, we now need to find what each component represents.\n\n\nPrincipal Component Analysis\n\npca_prep &lt;- pca_recipe |&gt;\n  prep()\n\npca_baked &lt;- pca_prep |&gt;\n  bake(new_data = NULL)\n\npca_tidy &lt;- tidy(pca_prep, 4, type = \"coef\") # tidy step 4 - the PCA step\n\nNow that we have the loadings for our component analysis, we can create a graph and interpret each component’s meaning.\n\npca_tidy |&gt;\n  filter(!(component %in% c(\"PC3\"))) |&gt;\n  mutate(terms = recode(terms, \n                        Quiteness = \"Quietness\", \n                        cleanliness = \"Cleanliness\", \n                        nurse_communication = \"Nurse Communication\", \n                        doctor_communication = \"Doctor Communication\", \n                        staff_responsiveness = \"Staff Responsivness\", \n                        communication_about_medicines = \"Communication About Medicines\",\n                        discharge_information = \"Discharge Information\",\n                        care_transition = \"Care Transition\"\n                        )) |&gt;\n  ggplot(aes(x = value, y = terms, fill = abs(value))) +\n  geom_col() +\n  labs(\n    title = \"Significant Principal Component \\nAnalysis Loadings\",\n    y = \"Terms\",\n    x = \"Values\"\n  ) +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(\n      hjust = 0.5, # center\n      face = \"bold\",\n      size = 20 # guessing this is okay for title\n    ),\n    axis.title.x = element_text( # sets only x-axis\n      size = 15\n    ),\n    axis.text.x = element_text(\n      size = 10\n    ),\n    axis.title.y = element_text( # sets only y-axis\n      size = 15\n    ),\n    axis.text.y = element_text(\n      size = 10\n    ),\n    legend.title = element_text( # legend title\n      size = 20\n    ),\n    legend.text = element_text( # Approved vs Not Approved\n      size = 16\n    )\n        ) +\n  scale_fill_gradient(low = \"black\", high = \"red\") +\n  facet_wrap(vars(component), nrow = 2)\n\n\n\n\nNow that we have the graphs of each component, we can interpret their meaning.\n\n\n\n\n\n\n\nComponent\nInterpretation\n\n\n\n\nPrincipal Component 1\nHighly rated\n\n\nPrincipal Component 2\nQuiet and clean vs discharge information\n\n\nPrincipal Component 4\nDoctor communication vs quiteness and discharge information\n\n\nPrincipal Component 5\nDoctor communication, discharge information, and care transition vs Staff responsiveness\n\n\nPrincipal Component 6\nStaff responsiveness, nurse communication, and doctor communication vs communication about medicines\n\n\nPrincipal Component 7\nCare transition vs doctor communication and discharge information\n\n\nPrincipal Component 8\nStaff responsiveness and care transition vs nurse communication"
  },
  {
    "objectID": "hospital_analysis.html#conclusion",
    "href": "hospital_analysis.html#conclusion",
    "title": "Hospital Analysis",
    "section": "Conclusion",
    "text": "Conclusion\nOur analysis has shown us that these are the factors that contribute to a change in a hospitals recommendation score. For example, if all other components are held constant, a change in principal component 2 will significantly affect a hospitals recommendation score. It is important to note that each subsequent component accounts for less of the overall variance between hospitals. This means that in descending order, when accurately interpreted, principal component 1 will account for fluctuation in the recommendation score the most, and principal component 8 will account for it the least. This analysis provides insights on what people value from hospitals, and what hospitals can do to provide better care.\n\nLimitations\nThere are a few limitations and ethical concerns that should be mentioned. These insights should not be used as a means of justification for hospitals to de-prioritze certain aspects of care in hopes of a better recommendation score. Not only would this be ineffective considering principle component 1, it would also have the potential to actively harm individuals. Another thing to consider is the reliability of the data. Hospital admins generally care a lot about how their hospital is rated. So much so, that some hospitals might not report accurate data. While the data from the Centers for Medicare and Medicaid should be reliable, it is important to note the possibility of data tampering."
  }
]